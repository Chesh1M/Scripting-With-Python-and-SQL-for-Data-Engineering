Setting up Scrapy
1) Create an empty directory for the Scrapy project and cd into the directory
2) Create virtual env for the project using 'python3 -m venv venv'
3) Activate the venv using 'source venv/bin/activate'
4) Install Scrapy using 'pip install scrapy'
('scrappy --help' to check list of commands available)
5) Create a scrapy project using 'scrapy startproject <insert project name here>'
6) Create spider
    - cd <project name>
    - scrapy genspider <insert spider name here> <insert website here for e.g. cve.mitre.org>


Using Scrapy
1) See all the different instances of a certain tag
e.g.: response.xpath('//table') # replace table with other tags
2) Combine with other functions like len() to see how many 'tables' there are in the page
e.g.: len(response.xpath('//table'))
3) Using a combination of different xpath arguements etc, find the required information on the page
e.g.: set child = response.xpath('//table')[10] # arbitrary number 10
      child.xpath('td//text()')[0].extract()
4) Once find out how to get to the data we want to parse through, edit the cve.py file 'parse' function
e.g.: 
def parse(self, response):
    for child in response.xpath('//table'):
        if len(child.xpath('tr')) > 100:
            table = child
            break
    for row in table.xpath('//tr'):
        try:
            print(row.xpath('td//text()')[0].extract())
        except IndexError:
            pass
5) Make sure the 'start_urls' is the correct link (actual page being parsed)
6) Run the spider using 'scrapy crawl <spider name>'
e.g.: scrapy crawl cve